from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig
import torch

# ğŸ”¸ LoRAã‚¢ãƒ€ãƒ—ã‚¿ã®ä¿å­˜å…ˆï¼ˆã‚ãªãŸã®ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ï¼‰
lora_adapter_path = "./educator/"

# ğŸ”¹ LoRAè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—
peft_config = PeftConfig.from_pretrained(lora_adapter_path)
base_model_name = peft_config.base_model_name_or_path  # ä¾‹: "meta-llama/Llama-2-7b-hf"

# ğŸ”¸ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆFP16 / GPUæœ€é©åŒ–ï¼‰
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    device_map="auto",           # GPU è‡ªå‹•å‰²å½“
    torch_dtype=torch.float16
)

# ğŸ”¹ LoRAã‚¢ãƒ€ãƒ—ã‚¿ã‚’ãƒãƒ¼ã‚¸ï¼ˆçµ±åˆãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆï¼‰
model = PeftModel.from_pretrained(base_model, lora_adapter_path)
model = model.merge_and_unload()  # LoRAã¨ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆ

# ğŸ”¸ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¯ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜
tokenizer = AutoTokenizer.from_pretrained(base_model_name)

# ğŸ”¹ æ¨è«–ï¼ˆä¾‹ï¼‰
instruction = "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã¦ãã ã•ã„"
input_text = "ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’æ ¼ç´ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆã¯ï¼Ÿ"
prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

# ğŸ”¸ å‡ºåŠ›è¡¨ç¤º
full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
if "### Response:" in full_response:
    response = full_response.split("### Response:")[-1].strip()
else:
    response = full_response.strip()

print(response)
