import os

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig
import torch


token = os.getenv("HUGGINGFACE_TOKEN")

base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-1B-Instruct",
    device_map="auto",           # GPU è‡ªå‹•å‰²å½“
    torch_dtype=torch.float16,
    use_auth_token=token
)

class agent():
    def __init__(self, folder_name):
        self.role = folder_name
        self.folder_path = f"./{folder_name}"
        self.instruction = {
                "educator":"ã“ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€ç”Ÿå¾’ã®å­¦ã³ã‚„ã™ã•ã‚„ç†è§£ã®æ·±ã¾ã‚Šã¨ã„ã†è¦³ç‚¹ã‹ã‚‰ã€ã©ã®ã‚ˆã†ã«ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã™ã¹ãã‹è€ƒãˆã¦ãã ã•ã„ã€‚æ•™å¸«ã¨ã—ã¦ã®çµŒé¨“ã‚’è¸ã¾ãˆã€ç¾å ´ã§ã®å®Ÿè·µå¯èƒ½æ€§ã«ã¤ã„ã¦ã‚‚è¿°ã¹ã¦ãã ã•ã„ã€‚",
                "engineer":"ã“ã®èª²é¡Œã‚’æŠ€è¡“çš„ã«è§£æ±ºã™ã‚‹ã«ã¯ã©ã®ã‚ˆã†ãªæ–¹æ³•ãŒã‚ã‚‹ã‹ã€AIã‚„ITã®æ´»ç”¨å¯èƒ½æ€§ã‚’ä¸­å¿ƒã«ã€åŠ¹ç‡æ€§ãƒ»å®Ÿç¾æ€§ãƒ»é©æ–°æ€§ã®è¦³ç‚¹ã‹ã‚‰ææ¡ˆã—ã¦ãã ã•ã„ã€‚ãƒ‡ãƒ¼ã‚¿ã‚„ãƒ­ã‚¸ãƒƒã‚¯ã«åŸºã¥ãèª¬æ˜ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚",
                "ethics_committee":"ã“ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚„æŠ€è¡“ã®å°å…¥ã«é–¢ã—ã¦ã€å€«ç†çš„ãªãƒªã‚¹ã‚¯ã‚„ç¤¾ä¼šçš„ãªå½±éŸ¿ã‚’ã©ã†è©•ä¾¡ã—ã¾ã™ã‹ï¼Ÿãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã€äººæ¨©ã€åè¦‹ã®å¯èƒ½æ€§ãªã©ã‚’è€ƒæ…®ã—ãŸä¸Šã§ã€å•é¡Œç‚¹ã¨ãã®å¯¾å‡¦ç­–ã‚’æŒ™ã’ã¦ãã ã•ã„ã€‚",
                "gurdian":"ã“ã®å–ã‚Šçµ„ã¿ãŒå­ã©ã‚‚ãŸã¡ã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ã€ä¿è­·è€…ã®è¦–ç‚¹ã‹ã‚‰è€ƒãˆã¦ãã ã•ã„ã€‚å®‰å…¨æ€§ã€æˆé•·ã¸ã®å½±éŸ¿ã€å®¶åº­ã§ã®ã‚µãƒãƒ¼ãƒˆã®ã—ã‚„ã™ã•ã«ã¤ã„ã¦ã‚‚æ„è¦‹ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚",
                "student":"ã“ã®è©±é¡Œã«ã¤ã„ã¦ã€å­¦ç”Ÿã®ç«‹å ´ã‹ã‚‰ãƒªã‚¢ãƒ«ãªæ„Ÿè¦šã§æ„è¦‹ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚å®Ÿéš›ã«ä½¿ã†ã¨ã—ãŸã‚‰ã©ã†æ„Ÿã˜ã‚‹ã‹ã€ä½•ãŒã†ã‚Œã—ãã¦ã€ä½•ãŒä¸å®‰ã‹ã€èº«è¿‘ãªçµŒé¨“ã‚‚äº¤ãˆã¦è©±ã—ã¦ãã ã•ã„ã€‚"
            }.get(folder_name)

        # ğŸ”¹ LoRAã‚¢ãƒ€ãƒ—ã‚¿ã‚’ãƒãƒ¼ã‚¸ï¼ˆçµ±åˆãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆï¼‰
        self.model = PeftModel.from_pretrained(base_model, self.folder_path)
        self.model = self.model.to(torch.float16).to("cuda" if torch.cuda.is_available() else "cpu")  # æ˜ç¤ºçš„ã«ãƒ­ãƒ¼ãƒ‰

        # ğŸ”¸ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¯ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜
        self.tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B-Instruct", use_auth_token=token)
    
        
    def predict(self, prompt):
        # ğŸ”¹ æ¨è«–ï¼ˆä¾‹ï¼‰
        prompt = f"### Instruction:\n{self.instruction}\n\n### Input:\n{prompt}\n\n### Response:\n"
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.5,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        # ğŸ”¸ å‡ºåŠ›è¡¨ç¤º
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        if "### Response:" in full_response:
            response = full_response.split("### Response:")[-1].strip()
        else:
            response = full_response.strip()

        return response